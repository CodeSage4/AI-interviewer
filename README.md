# üé§ AI Screen-Aware Technical Interviewer

A turn-based, screen-grounded AI interviewer that reasons from what is *currently visible on screen* ‚Äî not just uploaded documents ‚Äî and conducts a structured technical interview.

This system is designed to evaluate a presenter‚Äôs understanding of their own work by analyzing screenshots of slides, UI, diagrams, or code, and generating targeted, context-aware questions.

---

## üéØ Problem Statement

Traditional AI interview tools rely heavily on pre-uploaded PDFs or documents.  
This approach fails when:

- The presenter explains live diagrams  
- Code is shown dynamically  
- UI screens change in real time  
- Important context is visible *only on screen*

Our goal was to build an interviewer that:
- Grounds every question in **what is currently visible**
- Maintains a **coherent interview flow**
- Separates **understanding from probing**
- Provides **structured evaluation**, not vibes.

---

## üß† Core Design Philosophy

We deliberately **did NOT** build a real-time video system.

Instead, we chose a **high-quality turn-based loop**:

1. Presenter shows a screen  
2. System captures a screenshot  
3. OCR extracts visible text  
4. Lightweight analysis classifies the screen  
5. AI asks **two structured questions**  
6. Process repeats across multiple images  
7. Final rubric-based evaluation + radar chart

This avoids noisy streaming, improves reliability, and keeps the interview focused.

---

## üèóÔ∏è System Architecture

Our pipeline has **three explicit layers**:


---

## üîç How Each Component Works

### 1Ô∏è‚É£ Screen Capture
The presenter uploads a screenshot of what they are currently showing.

We intentionally avoid live video because:
- OCR is more reliable on static images  
- Noise is reduced  
- The system becomes deterministic and debuggable  

---

### 2Ô∏è‚É£ OCR Layer (EasyOCR)

For every image we:
- Run EasyOCR  
- Extract visible text  
- Keep confidence scores  
- Clean and structure output for downstream reasoning  

This allows the system to reason about:
- Code snippets  
- UI labels  
- Diagram text  
- Slide bullets  

---

### 3Ô∏è‚É£ Screen Understanding Layer

Instead of dumping raw OCR into an LLM, we perform lightweight analysis:

We classify the screen as one of:
- **CODE** ‚Üí analyze variables, control flow, and potential bugs  
- **ARCHITECTURE** ‚Üí extract components like API, DB, Cache, Services  
- **UI** ‚Üí identify interaction risks (blur, scale, alignment, lighting)  
- **SLIDES** ‚Üí extract key claims and assumptions  

This is what makes the system *screen-aware*, not just text-aware.

---

### 4Ô∏è‚É£ Structured Interview Logic (Two Questions Per Image)

For each screenshot, the AI follows a deliberate pattern:

#### **Q1 ‚Äî Understanding (breadth)**
Goal: Align on what the screen represents.

Example:
> ‚ÄúAt a high level, how does this ‚ÄòUpload Foot Image on A4 paper‚Äô step fit into your pipeline?‚Äù

#### **Q2 ‚Äî Probing (depth)**
Goal: Explore risks, edge cases, and design tradeoffs.

Example:
> ‚ÄúIf the A4 sheet is rotated or partially cropped, how does your system recover the correct scale?‚Äù

This creates a **coherent conversation**, not disconnected Q&A.

---

### 5Ô∏è‚É£ Stateful Memory

The system retains:
- Previous questions  
- Student answers  
- Current intent (understand vs probe)  

This ensures continuity across turns rather than random questioning.

---

## üìä Evaluation & Scoring

At the end of the interview, the system generates a radar chart based on four axes:

### ‚úî Technical Depth  
Measures how many real technical concepts the student used.

### ‚úî Clarity  
Rewards structured, well-reasoned explanations.

### ‚úî Originality  
Detects whether the student proposed alternatives or improvements.

### ‚úî Implementation Understanding  
Checks whether the student identified bugs, risks, or edge cases.

Scores are heuristic but **grounded in reasoning signals**, not word count.

---

## ‚úÖ Why This Design is Strong

We intentionally chose:
- **Turn-based capture** over streaming  
- **OCR + human summary** over pure vision models  
- **Light analysis before LLM** instead of raw text dumping  
- **Two-question structure** instead of single-shot queries  

These are **engineering decisions, not shortcuts.**

---

## ‚ö†Ô∏è Limitations

We are transparent about what this system does *not* do:

- No true multimodal vision reasoning  
- Diagrams without text still need a human summary  
- OCR can struggle with handwriting or low-quality images  
- LLM questions may still be imperfect  

### Future Work:
- Add GPT-4o / vision models  
- Better diagram parsing  
- Automatic diagram summarization  
- Live speech support  

---

## üéØ Final Goal

This project is not about perfect AI ‚Äî it is about:

> A reliable, explainable, screen-grounded interview loop that reasons from what is visible, rather than relying solely on uploaded documents.

---

## üõ†Ô∏è Tech Stack

- **Frontend:** Streamlit  
- **OCR:** EasyOCR  
- **LLM:** Mistral via Ollama (offline)  
- **Processing:** OpenCV + NumPy  
- **Visualization:** Matplotlib (radar chart)

---
